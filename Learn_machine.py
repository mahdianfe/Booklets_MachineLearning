# 🚩 ⛑️ ❌ ❓‼️⁉️💯 ❎✅☑️✖️✔️🟰  ♦️🔶🔷 ⭕🚫⛔🔴⚪⚫🔵 👆🏻👇🏻👉🏻👈🏻 🤓🙃😉😄😎😏  😶😮😵‍💫🫨️ 😫🤨🧐 💭
# 1️⃣2️⃣3️⃣4️⃣5️⃣6️⃣7️⃣8️⃣9️⃣🔟0️⃣🆘📍📌️🖊️️✒️🔎🔦🔍🗝️🔑🎯🤷🏻🤷🏻‍♂️🤷🏻‍♀️🤦🏻🤦🏻‍♂️🤦🏻‍♀️🪴🌼

###_______________________________________________________________________________________________
"""نصب کتابخانه scikit-learn: """
# pip install -U scikit-learn

# 🚫 نکته: زمانی که -U را نمیذاشتم کتابخانه نصب نمیشد و خطا میداد
# شایدم به این خاطر بود که ادرس مفسرم ادرس محیط مجازی ای نبود که ترمینالم داشت
# خطا:
# [notice] A new release of pip is available: 24.2 -> 25.0.1
# [notice] To update, run: python.exe -m pip install --upgrade pip
# ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.
#     unknown package:
#         Expected sha256 70b1d7e85b1c96383f872a519b3375f92f14731e279a7b4c6cfd650cf5dffc52
#              Got        97f7d47b477a29f383ed44fca9f1bc5ffdd56a739369aefae55837d3dbc19316
###_______________________________________________________________________________________________

"""ایمپورت کردن کتابخانه ها"""
import numpy as np
import pandas as pd

# مت پلات لیب اینبار در پایچارم خطا میداد بنابراین به این شکل کتابخانه را ایمپورت کردم درست شد
import matplotlib
matplotlib.use('TkAgg')  # یا از 'Agg' استفاده کنید اگر TkAgg کار نکرد
# #  matplotlib برای نمایش نمودارها از چیزی به نام "backend" استفاده می‌کند. بک‌اند همان سیستمی است که matplotlib برای رسم و نمایش گراف‌ها در صفحه استفاده می‌کند.
# # در PyCharm، وقتی از plt.show() استفاده می‌کنی، ممکن است PyCharm خودش بک‌اند خاصی مثل "InterAgg" را به کار بگیرد که برای نمایش نمودار در پنجره‌های جداگانه طراحی نشده است.
# TkAgg: از Tkinter استفاده می‌کند و معمولاً در بیشتر سیستم‌ها به‌خوبی کار می‌کند.
import matplotlib.pyplot as plt

###_______________________________________________________________________________________________

"""باز کردن فایل دیتا ست"""
df=pd.read_csv('diabetes.csv')
print(df.head(), "\n" ,"**********************************")
print(df.shape, "\n" ,"**********************************")
print(df.describe(), "\n" ,"**********************************")
print(df.info(), "\n" ,"**********************************")
print(df.value_counts(), "\n" ,"**********************************")
print(df["Outcome"].value_counts(), "\n" ,"**********************************")
###_______________________________________________________________________________________________

"""ساخت متغیر از جدول"""

X=df.drop("Outcome",axis=1)
#نکته 1 : عبارت axis=1 یعنی تمام ستون منظور است

y=df["Outcome"]

###___________________
# نکته مهم 1 :
#عبارت X: شامل تمام ویژگی‌ها (feature) یا خصوصیات جدول شما است، به جز ستون "Outcome".
# در واقع، X ماتریسی است که اطلاعات مربوط به تمام متغیرهای مستقل را در خود جای داده است.
# y: شامل ستون "Outcome" است، که همان لیبل یا برچسب‌های شما (0 و 1) است. این ستون نشان می‌دهد
# که آیا فرد مریض است (1) یا خیر (0). به عبارتی، y متغیر وابسته یا هدف شما است که می‌خواهید آن را پیش‌بینی کنید.

###___________________
# نکته مهم 2 :
# چرا از ایکس بزرگ استفاده شده اما در مورد خروجی از y کوچک استفاده میشه ؟


# جواب:
# دلایل استفاده از X بزرگ:
#     ماتریس ویژگی‌ها:
#         در یادگیری ماشین، معمولاً داده‌های ورودی (ویژگی‌ها) به صورت یک ماتریس نمایش داده می‌شوند. ماتریس‌ها معمولاً با حروف بزرگ نمایش داده می‌شوند.
#         X به عنوان یک ماتریس تلقی می‌شود که هر سطر آن یک نمونه داده و هر ستون آن یک ویژگی را نشان می‌دهد.
#     تمایز از بردار هدف:
#         استفاده از حرف بزرگ به تمایز ماتریس ویژگی‌ها از بردار هدف (y) کمک می‌کند.
#

# دلایل استفاده از y کوچک:
#     بردار هدف:
#         داده‌های خروجی (برچسب‌ها یا مقادیر هدف) معمولاً به صورت یک بردار نمایش داده می‌شوند.
#         بردارها معمولاً با حروف کوچک نمایش داده می‌شوند.
#         y به عنوان یک بردار تلقی می‌شود که هر عنصر آن برچسب مربوط به یک نمونه داده را نشان می‌دهد.
#     متغیر وابسته:
#         y نشان‌دهنده متغیر وابسته یا هدف است که می‌خواهیم آن را پیش‌بینی کنیم.
#         در آمار و یادگیری ماشین، متغیرهای وابسته معمولاً با حروف کوچک نمایش داده می‌شوند.
###_______________________________________________________________________________________________
"""تبدیل X , y به آرایه"""
X=np.array(X)
y=np.array(y)
print(X)
print(y)


###_______________________________________________________________________________________________
"""Normalize"""
# برای اینکه بخاطر بزرگ بودن بعضی از اعداد ستونها نسبت به بقیه ستونها مدل تفاوتی قائل نشود
# همه را تبدیل به اعدادی می کنیم که بین صفر و یک شوند
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X=scaler.fit_transform(X)
###_______________________________________________________________________________________________
"""Test /  Train 1 """
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
# نکته: یعنی 20 درصد داده ها را تست در نظر بگیر
# نکته: به طور خلاصه، random_state یک مقدار عددی است که به عنوان "seed" برای مولد اعداد تصادفی استفاده می‌شود. این کار تضمین می‌کند که:
# 1. تکرارپذیری: اگر random_state را با یک مقدار ثابت تنظیم کنید، هر بار که کد را اجرا می‌کنید،
# .همان تقسیم داده‌ها را دریافت خواهید کرد
# فرض کنید دو مدل یادگیری ماشین مختلف (مثلاً مدل A و مدل B) را برای یک مسئله خاص آموزش می‌دهید.
# برای اینکه بتوانید به طور منصفانه عملکرد این دو مدل را با هم مقایسه کنید،
# باید آنها را روی یک مجموعه داده آزمایشی یکسان ارزیابی کنید.
# 2.کنترل تصادفی بودن: بدون random_state، هر بار که train_test_split را اجرا می‌کنید، تقسیم داده‌ها متفاوت خواهد بود.
# این امر می‌تواند منجر به نتایج متفاوتی در ارزیابی مدل شود.

###_______________________________________________________________________________________________
# نکته بسیار مهم
""" 
گاهی لازمه که:
#  الف. روی نمونه خودمان طبقه بندی انجام دهیم 
و ب. سپس تست و ترین را از هم جدا کنیم
# و ج. حذف ستونی که برای طبقه بندی ایجاد کرده بودیم

 """
# نمونه‌برداری طبقه‌بندی شده (Stratified Sampling)
# در نمونه‌برداری طبقه‌بندی شده، جمعیت را به گروه‌های مشابه (طبقات) تقسیم می‌کنیم و سپس از هر گروه به صورت جداگانه نمونه‌گیری می‌کنیم.
# این کار باعث می‌شود که نمونه ما نماینده بهتری از کل جمعیت باشد و نظرات همه گروه‌ها در نمونه ما منعکس شود.
#
# فرض کنید یک جعبه دارید که داخل آن تیله‌های رنگی (آبی، قرمز، سبز) وجود دارد

# با توجه به اینکه تعداد هر تیله ممکنه متفاوت باشد یا حتی نباشد
# اگر شما 20 درصد را از تمام اینها برداری ممکنه بعضی رنگ ها در نظر گرفته نشه و یا مقدارش فرق کنه.
# به همین دلیل، نمونه‌برداری طبقه‌بندی شده تأکید می‌کند که باید از هر طبقه (رنگ) به نسبت تعداد آن طبقه نمونه‌برداری کنیم.
# مثلا در این مثال:
# حجم کل نمونه:
#     20 درصد از 100 تیله = 20 تیله
# حجم نمونه قرمز:
#     از آنجایی که می‌خواهیم نسبت 30 درصد قرمز را در نمونه حفظ کنیم، باید 20 درصد از 30 تیله قرمز را انتخاب کنیم.
#     20 درصد از 30 تیله = 6 تیله
###_____________________
"""نمایش یک ستون دلخواه برای ایجاد طبقه بندی """
# plt.plot(df["BMI"])
# plt.show()
# # نمییدونم چرا این قسمت در فایل .py ارور داد اما در جوپیتر نداد
###_____________________
"""
الف. ایجاد نمونه
 طبقه بندی با استفاده از پانداس
برای حالتی که  مقدار پیوسته باشد
خودم: دسته ها دقیق نیست خودم همینجوری نوشتم
"""
# یک دسته با نام BMI_categories به جدول اضافه میکنیم
# #  نکته مهم مهم : البته بعد از اینکه اسپلیت و .. را انجام دادیم این ستون را حذف میکنیم

df["BMI_categories"] = pd.cut(df["BMI"],
       bins=[-float('inf'),10,20,40,50,70,float('inf')],
       #bins بازه هایی است که میخواهیم به دسته های مختف تقسیم شود
       #عبارت np.inf هم یعنی بینهایت
       #طول بازه ها میتواند یکسان نباشد
       labels=[0,1,2,3,4,5])
        #برای هر ستون یک لیبل و نام در نظر میگیریم
        #6 تا لیبل انتخاب کردیم چون بین هر عدد را یک دسته میگیریم
###_____________________
# print(df["BMI_categories"])
# print(df["BMI"])
# print(df)
###_____________________

"""چک کردن اینکه در هر کتگوری از  BMI چه تعداد است"""
df["BMI_categories"].value_counts()

###_____________________

"""نمایش نمودار این ستون"""
df["BMI_categories"].value_counts().plot.bar(grid=True)
# جدول از ماکسیمم رسم میشه. برای اینکه از ابتدا اندیس 1 رسم بشه میتوان sort_index() را هم قرار دهیم

###_____________________
# # بیشتر مطالعه شود
"""
روش اول ب
Test /  Train 2
"""
# from sklearn.model_selection import StratifiedShuffleSplit
# sss=StratifiedShuffleSplit(n_splits=1,test_size=0.2 , random_state=0)
# #  عبارت n_splits تعداد دسته بندی مختلف را میده
# #عبارت test_size=0.2 یعنی 20 درصد نمونه ها  را برای تست درنظر بگیر
# #عبارت  random_state مثل  seed در رندوم عمل میکنه
#
# stratified_split=[]
# for train_index , test_index in  sss.split(df, df["BMI_categories"]):
# # df را به عنوان ایکس بهش میدیم و ستونی که خودمان درست کردیم را به عنوان  y
# # چون نمونه تست را طبق این دسته بندی میخواهیم انجام دهیم
#        stratified_train_set_n = df.iloc[train_index]
#        #n را گذاشتیم که بدونیم به تعداد اسپلیت ها داره انجام میشه
#        stratified_test_set_n = df.iloc[test_index]
#        stratified_split.append([stratified_train_set_n,stratified_test_set_n])
# stratified_train_set , stratified_test_set = stratified_split[0]
# # # اندیس صفرم مانند بقیه اندیس ها یک لیست است که اولی شامل ترین و دومی تست هستش
#
# print(" stratified_train_set **********************************")
# print(stratified_train_set)
# print(" stratified_test_set **********************************")
# print(stratified_test_set)

###_____________________
# نکته بسیار مهم
""" روش دوم ب
 Test /  Train 3
#  روش راحت تر برای اسپلیت ترین و تست برای طبقه بندی که ایجاد کردیم
# یعنی خود تابع اسپلیت در کتابخانه سایکیت لرن یه پارامتر به نام stratify دارد 
"""
stratified_train_set, stratified_test_set = train_test_split(df,
                                               test_size=0.2,
                                               stratify=df["BMI_categories"],
                                               random_state=40)
print(stratified_train_set, "\n" ," ***********************************")
###_______________________________________________________________________________________________
""" و ج. حذف ستونی که برای طبقه بندی ایجاد کرده بودیم
"""