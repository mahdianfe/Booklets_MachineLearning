# نکته برداری از سایت های:

# 1. https://www.youtube.com/watch?v=Nv4tDbCgMK0&list=PLs74m7pSWiut8T1HUKuBBLI7ldMMYZurw&index=2

#  2. بخش Stratified Sampling از سایت:
# https://faradars.org/courses/machine-learning-using-python-fvpht0091

#  3. و چت جی پی تی و جیمینی

 
###_______________________________________________________________________________________________


"""نصب کتابخانه scikit-learn: """
# pip install -U scikit-learn

# 🚫 نکته: زمانی که -U را نمیذاشتم کتابخانه نصب نمیشد و خطا میداد
# شایدم به این خاطر بود که ادرس مفسرم ادرس محیط مجازی ای نبود که ترمینالم داشت
# خطا:
# [notice] A new release of pip is available: 24.2 -> 25.0.1
# [notice] To update, run: python.exe -m pip install --upgrade pip
# ERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE. If you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.
#     unknown package:
#         Expected sha256 70b1d7e85b1c96383f872a519b3375f92f14731e279a7b4c6cfd650cf5dffc52
#              Got        97f7d47b477a29f383ed44fca9f1bc5ffdd56a739369aefae55837d3dbc19316
###_______________________________________________________________________________________________

"""ایمپورت کردن کتابخانه ها"""
import numpy as np
import pandas as pd

# مت پلات لیب اینبار در پایچارم خطا میداد بنابراین به این شکل کتابخانه را ایمپورت کردم درست شد
import matplotlib
matplotlib.use('TkAgg')  # یا از 'Agg' استفاده کنید اگر TkAgg کار نکرد
# #  matplotlib برای نمایش نمودارها از چیزی به نام "backend" استفاده می‌کند. بک‌اند همان سیستمی است که matplotlib برای رسم و نمایش گراف‌ها در صفحه استفاده می‌کند.
# # در PyCharm، وقتی از plt.show() استفاده می‌کنی، ممکن است PyCharm خودش بک‌اند خاصی مثل "InterAgg" را به کار بگیرد که برای نمایش نمودار در پنجره‌های جداگانه طراحی نشده است.
# TkAgg: از Tkinter استفاده می‌کند و معمولاً در بیشتر سیستم‌ها به‌خوبی کار می‌کند.
import matplotlib.pyplot as plt

###_______________________________________________________________________________________________

"""باز کردن فایل دیتا ست"""
df=pd.read_csv('diabetes.csv')
print(df.head(), "\n" ,"**********************************")
print(df.shape, "\n" ,"**********************************")
print(df.describe(), "\n" ,"**********************************")
print(df.info(), "\n" ,"**********************************")
print(df.value_counts(), "\n" ,"**********************************")
print(df["Outcome"].value_counts(), "\n" ,"**********************************")
###_______________________________________________________________________________________________

"""ساخت متغیر از جدول"""

X=df.drop("Outcome",axis=1)
#نکته 1 : عبارت axis=1 یعنی تمام ستون منظور است

y=df["Outcome"]

###___________________
# نکته مهم 1 :
#عبارت X: شامل تمام ویژگی‌ها (feature) یا خصوصیات جدول شما است، به جز ستون "Outcome".
# در واقع، X ماتریسی است که اطلاعات مربوط به تمام متغیرهای مستقل را در خود جای داده است.
# y: شامل ستون "Outcome" است، که همان لیبل یا برچسب‌های شما (0 و 1) است. این ستون نشان می‌دهد
# که آیا فرد مریض است (1) یا خیر (0). به عبارتی، y متغیر وابسته یا هدف شما است که می‌خواهید آن را پیش‌بینی کنید.

###___________________
# نکته مهم 2 :
# چرا از ایکس بزرگ استفاده شده اما در مورد خروجی از y کوچک استفاده میشه ؟


# جواب:
# دلایل استفاده از X بزرگ:
#     ماتریس ویژگی‌ها:
#         در یادگیری ماشین، معمولاً داده‌های ورودی (ویژگی‌ها) به صورت یک ماتریس نمایش داده می‌شوند. ماتریس‌ها معمولاً با حروف بزرگ نمایش داده می‌شوند.
#         X به عنوان یک ماتریس تلقی می‌شود که هر سطر آن یک نمونه داده و هر ستون آن یک ویژگی را نشان می‌دهد.
#     تمایز از بردار هدف:
#         استفاده از حرف بزرگ به تمایز ماتریس ویژگی‌ها از بردار هدف (y) کمک می‌کند.
#

# دلایل استفاده از y کوچک:
#     بردار هدف:
#         داده‌های خروجی (برچسب‌ها یا مقادیر هدف) معمولاً به صورت یک بردار نمایش داده می‌شوند.
#         بردارها معمولاً با حروف کوچک نمایش داده می‌شوند.
#         y به عنوان یک بردار تلقی می‌شود که هر عنصر آن برچسب مربوط به یک نمونه داده را نشان می‌دهد.
#     متغیر وابسته:
#         y نشان‌دهنده متغیر وابسته یا هدف است که می‌خواهیم آن را پیش‌بینی کنیم.
#         در آمار و یادگیری ماشین، متغیرهای وابسته معمولاً با حروف کوچک نمایش داده می‌شوند.
###_______________________________________________________________________________________________
"""تبدیل X , y به آرایه"""
X=np.array(X)
y=np.array(y)
print(X)
print(y)


###_______________________________________________________________________________________________
"""Normalize"""
# برای اینکه بخاطر بزرگ بودن بعضی از اعداد ستونها نسبت به بقیه ستونها مدل تفاوتی قائل نشود
# همه را تبدیل به اعدادی می کنیم که بین صفر و یک شوند
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X=scaler.fit_transform(X)
###_______________________________________________________________________________________________
"""Test /  Train 1 """
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
# نکته: یعنی 20 درصد داده ها را تست در نظر بگیر
# نکته: به طور خلاصه، random_state یک مقدار عددی است که به عنوان "seed" برای مولد اعداد تصادفی استفاده می‌شود. این کار تضمین می‌کند که:
# 1. تکرارپذیری: اگر random_state را با یک مقدار ثابت تنظیم کنید، هر بار که کد را اجرا می‌کنید،
# .همان تقسیم داده‌ها را دریافت خواهید کرد
# فرض کنید دو مدل یادگیری ماشین مختلف (مثلاً مدل A و مدل B) را برای یک مسئله خاص آموزش می‌دهید.
# برای اینکه بتوانید به طور منصفانه عملکرد این دو مدل را با هم مقایسه کنید،
# باید آنها را روی یک مجموعه داده آزمایشی یکسان ارزیابی کنید.
# 2.کنترل تصادفی بودن: بدون random_state، هر بار که train_test_split را اجرا می‌کنید، تقسیم داده‌ها متفاوت خواهد بود.
# این امر می‌تواند منجر به نتایج متفاوتی در ارزیابی مدل شود.

###_______________________________________________________________________________________________
# نکته بسیار مهم
""" 
گاهی لازمه که:
#  الف. روی نمونه خودمان طبقه بندی انجام دهیم 
و ب. سپس تست و ترین را از هم جدا کنیم
# و ج. حذف ستونی که برای طبقه بندی ایجاد کرده بودیم

 """
# نمونه‌برداری طبقه‌بندی شده (Stratified Sampling)
# در نمونه‌برداری طبقه‌بندی شده، جمعیت را به گروه‌های مشابه (طبقات) تقسیم می‌کنیم و سپس از هر گروه به صورت جداگانه نمونه‌گیری می‌کنیم.
# این کار باعث می‌شود که نمونه ما نماینده بهتری از کل جمعیت باشد و نظرات همه گروه‌ها در نمونه ما منعکس شود.
#
# فرض کنید یک جعبه دارید که داخل آن تیله‌های رنگی (آبی، قرمز، سبز) وجود دارد

# با توجه به اینکه تعداد هر تیله ممکنه متفاوت باشد یا حتی نباشد
# اگر شما 20 درصد را از تمام اینها برداری ممکنه بعضی رنگ ها در نظر گرفته نشه و یا مقدارش فرق کنه.
# به همین دلیل، نمونه‌برداری طبقه‌بندی شده تأکید می‌کند که باید از هر طبقه (رنگ) به نسبت تعداد آن طبقه نمونه‌برداری کنیم.
# مثلا در این مثال:
# حجم کل نمونه:
#     20 درصد از 100 تیله = 20 تیله
# حجم نمونه قرمز:
#     از آنجایی که می‌خواهیم نسبت 30 درصد قرمز را در نمونه حفظ کنیم، باید 20 درصد از 30 تیله قرمز را انتخاب کنیم.
#     20 درصد از 30 تیله = 6 تیله
###_____________________
"""نمایش یک ستون دلخواه برای ایجاد طبقه بندی """
# plt.plot(df["BMI"])
# plt.show()
# # نمییدونم چرا این قسمت در فایل .py ارور داد اما در جوپیتر نداد
###_____________________
"""Stratified Sampling"""
"""
الف. ایجاد نمونه
 طبقه بندی با استفاده از پانداس
برای حالتی که  مقدار پیوسته باشد
خودم: دسته ها دقیق نیست خودم همینجوری نوشتم
"""
# یک دسته با نام BMI_categories به جدول اضافه میکنیم
# #  نکته مهم مهم : البته بعد از اینکه اسپلیت و .. را انجام دادیم این ستون را حذف میکنیم

df["BMI_categories"] = pd.cut(df["BMI"],
       bins=[-float('inf'),10,20,40,50,70,float('inf')],
       #bins بازه هایی است که میخواهیم به دسته های مختف تقسیم شود
       #عبارت np.inf هم یعنی بینهایت
       #طول بازه ها میتواند یکسان نباشد
       labels=[0,1,2,3,4,5])
        #برای هر ستون یک لیبل و نام در نظر میگیریم
        #6 تا لیبل انتخاب کردیم چون بین هر عدد را یک دسته میگیریم
###_____________________
# print(df["BMI_categories"])
# print(df["BMI"])
# print(df)
###_____________________

"""چک کردن اینکه در هر کتگوری از  BMI چه تعداد است"""
df["BMI_categories"].value_counts()

###_____________________

"""نمایش نمودار این ستون"""
df["BMI_categories"].value_counts().plot.bar(grid=True)
# جدول از ماکسیمم رسم میشه. برای اینکه از ابتدا اندیس 1 رسم بشه میتوان sort_index() را هم قرار دهیم

###_____________________
# # بیشتر مطالعه شود
"""
روش اول ب
Test /  Train 2
"""
# from sklearn.model_selection import StratifiedShuffleSplit
# sss=StratifiedShuffleSplit(n_splits=1,test_size=0.2 , random_state=0)
# #  عبارت n_splits تعداد دسته بندی مختلف را میده
# #عبارت test_size=0.2 یعنی 20 درصد نمونه ها  را برای تست درنظر بگیر
# #عبارت  random_state مثل  seed در رندوم عمل میکنه
#
# stratified_split=[]
# for train_index , test_index in  sss.split(df, df["BMI_categories"]):
# # df را به عنوان ایکس بهش میدیم و ستونی که خودمان درست کردیم را به عنوان  y
# # چون نمونه تست را طبق این دسته بندی میخواهیم انجام دهیم
#        stratified_train_set_n = df.iloc[train_index]
#        #n را گذاشتیم که بدونیم به تعداد اسپلیت ها داره انجام میشه
#        stratified_test_set_n = df.iloc[test_index]
#        stratified_split.append([stratified_train_set_n,stratified_test_set_n])
# stratified_train_set , stratified_test_set = stratified_split[0]
# # # اندیس صفرم مانند بقیه اندیس ها یک لیست است که اولی شامل ترین و دومی تست هستش
#
# print(" stratified_train_set **********************************")
# print(stratified_train_set)
# print(" stratified_test_set **********************************")
# print(stratified_test_set)

###_____________________
# نکته بسیار مهم
""" روش دوم ب
 Test /  Train 3
#  روش راحت تر برای اسپلیت ترین و تست برای طبقه بندی که ایجاد کردیم
# یعنی خود تابع اسپلیت در کتابخانه سایکیت لرن یه پارامتر به نام stratify دارد 
"""
stratified_train_set, stratified_test_set = train_test_split(df,
                                               test_size=0.2,
                                               stratify=df["BMI_categories"],
                                               random_state=40)
print(stratified_train_set, "\n" ," ***********************************")
###_______________________________________________________________________________________________
""" و ج. حذف ستونی که برای طبقه بندی ایجاد کرده بودیم
"""
stratified_train_set = stratified_train_set.drop("BMI_categories",axis=1)
stratified_test_set = stratified_test_set.drop("BMI_categories",axis=1)

###_______________________________________________________________________________________________
"""آماده سازی برای اینکه کد شما با استفاده از مجموعه‌های داده ترین و تست که از نمونه‌برداری طبقه‌بندی شده به دست آمده‌اند کار کند"""
# جداسازی ویژگی‌ها (X) و متغیر هدف (y) در مجموعه‌های ترین و تست طبقه‌بندی شده:
# بعد از حذف ستون "BMI_categories"، باید ستون "Outcome" را از مجموعه‌های ترین و تست جدا کنید
# تا متغیرهای X و y را برای هر مجموعه داشته باشید.

# جداسازی ویژگی‌ها و متغیر هدف برای مجموعه ترین
X_train_stratified = stratified_train_set.drop("Outcome", axis=1)
y_train_stratified = stratified_train_set["Outcome"]

# جداسازی ویژگی‌ها و متغیر هدف برای مجموعه تست
X_test_stratified = stratified_test_set.drop("Outcome", axis=1)
y_test_stratified = stratified_test_set["Outcome"]

# تبدیل به آرایه (اگر نیاز باشد)
X_train_stratified = np.array(X_train_stratified)
y_train_stratified = np.array(y_train_stratified)
X_test_stratified = np.array(X_test_stratified)
y_test_stratified = np.array(y_test_stratified)
#چرا به آرایه احتیاج داریم؟
# در یادگیری ماشین، الگوریتم‌ها معمولاً با داده‌هایی کار می‌کنند که به صورت عددی و ساختاریافته هستند. آرایه‌ها (به خصوص آرایه‌های NumPy) یک روش کارآمد و استاندارد برای ذخیره و دسترسی به این نوع داده‌ها هستند.
#
#     کارایی: کتابخانه‌های یادگیری ماشین مانند scikit-learn و TensorFlow برای کار با آرایه‌های NumPy بهینه شده‌اند. محاسبات روی آرایه‌ها بسیار سریع‌تر از لیست‌های پایتون انجام می‌شوند.
#     سازگاری: بسیاری از توابع و کلاس‌های موجود در این کتابخانه‌ها ورودی‌هایی را می‌پذیرند که به صورت آرایه هستند.
#     ساختار: آرایه‌ها ساختار مشخصی دارند که برای نمایش داده‌های جدولی (مانند ویژگی‌ها و برچسب‌ها) مناسب است.


###_______________________________________________________________________________________________
"""
#  یادگیری نظارت شده از روش کلسیفیکشن:
# 1. الگوریتم بیز ساده (Naïve Bayes)
"""
# #خلاصه:
#  الگوریتم بیز ساده یک روش سریع و کارآمد برای دسته‌بندی داده‌ها است که بر پایه قضیه بیز و فرض استقلال ویژگی‌ها کار می‌کند. این الگوریتم با محاسبه احتمالات پیشین و شرطی و استفاده از قضیه بیز، احتمال تعلق یک نمونه به هر کلاس را محاسبه کرده و کلاسی با بالاترین احتمال را انتخاب می‌کند. علی‌رغم فرض ساده‌سازی کننده خود، در بسیاری از موارد عملکرد خوبی دارد و به ویژه برای دسته‌بندی متن مناسب است.
#
#
# الگوریتم بیز ساده انواع مختلفی دارد که بر اساس نوع داده‌ها و توزیع ویژگی‌ها انتخاب می‌شوند:
#     Gaussian Naive Bayes: برای ویژگی‌هایی که توزیع نرمال دارند.
#     Multinomial Naive Bayes: برای داده‌های گسسته مانند تعداد کلمات در متن.
#     Bernoulli Naive Bayes: برای ویژگی‌های باینری (صفر و یک).
#
#
# مزایا:
#     -ساده و آسان برای پیاده‌سازی.
#     -بسیار سریع و کارآمد، حتی برای داده‌های بزرگ.
#     -عملکرد خوبی در دسته‌بندی متن دارد.
#     -نیاز به داده‌های آموزشی کمی دارد.
#
#
# معایب:
#     - فرض استقلال ویژگی‌ها اغلب نادرست است.
#     -اگر یک ویژگی برای یک کلاس خاص در داده‌های آموزشی وجود نداشته باشد، احتمال شرطی صفر می‌شود و ممکن است مشکل ایجاد کند.
#     -عملکرد آن ممکن است در برخی موارد از الگوریتم‌های پیچیده‌تر پایین‌تر باشد.
#
# مثلا:
#  قبلا موز و سیب و هندوانه را دیدیم. حالا میگن یک میوه 5 کیلویی داریم بگو چه میوه است. خب معلومه که هندوانه هستش
#
#  فرمول بیز در ریاضی:
# P(A|B) = [P(B|A) * P(A)] / P(B)
#
###_____________________

from sklearn.naive_bayes import GaussianNB
model=GaussianNB()
model.fit(X_train_stratified,y_train_stratified)
# عبارت‌هایی که داخل پرانتز متد fit() قرار می‌گیرند، داده‌های آموزشی و برچسب‌های آموزشی هستند.
    # آرگومان اول: معمولاً ماتریس ویژگی‌های آموزشی (X_train_stratified) است. این ماتریس شامل تمام ویژگی‌هایی است که مدل برای یادگیری الگوها از آن‌ها استفاده می‌کند. هر سطر در این ماتریس نشان‌دهنده یک نمونه آموزشی است و هر ستون نشان‌دهنده یک ویژگی.
    # آرگومان دوم: معمولاً بردار برچسب‌های آموزشی (y_train_stratified) است. این بردار شامل برچسب یا مقدار هدف متناظر با هر نمونه آموزشی در X_train_stratified است.


y_pred_train = model.predict(X_train_stratified)
y_pred_test = model.predict(X_test_stratified)
# متد predict() انتظار دارد که ورودی آن ویژگی‌های نمونه‌هایی باشد که می‌خواهید برچسب آن‌ها را پیش‌بینی کنید.

###_______________________________________________________________________________________________

"""مفاهیم کلیدی ارزیابی مدل دسته‌بندی"""

"""# 1. Confusion Matrix (ماتریس درهم‌ریختگی):"""
#
#     تعریف: یک جدول که نتایج پیش‌بینی‌های مدل را در مقابل مقادیر واقعی نشان می‌دهد.
#     خلاصه: به ما کمک می‌کند تا ببینیم مدل در دسته‌بندی هر کلاس چگونه عمل کرده است و چه نوع اشتباهاتی (مثبت اشتباه، منفی اشتباه) مرتکب شده است.
#     اجزا:
#         True Positive (TP): نمونه‌های مثبت که به درستی به عنوان مثبت پیش‌بینی شده‌اند.
#         True Negative (TN): نمونه‌های منفی که به درستی به عنوان منفی پیش‌بینی شده‌اند.
#         False Positive (FP): نمونه‌های منفی که به اشتباه به عنوان مثبت پیش‌بینی شده‌اند.
#         False Negative (FN): نمونه‌های مثبت که به اشتباه به عنوان منفی پیش‌بینی شده‌اند.
#
"""# 2. Accuracy Score (امتیاز دقت):"""
#
#     تعریف: نسبت تعداد پیش‌بینی‌های درست به کل تعداد پیش‌بینی‌ها.
#     خلاصه: نشان می‌دهد مدل در مجموع چه نسبتی از نمونه‌ها را به درستی دسته‌بندی کرده است.
#     نکته: در مجموعه‌های داده نامتعادل (که تعداد نمونه‌های یک کلاس بسیار بیشتر از کلاس دیگر است)، دقت می‌تواند گمراه‌کننده باشد.
#
""" 3. Precision Score (امتیاز دقت مثبت):"""
#
#     تعریف: نسبت تعداد نمونه‌هایی که مدل به درستی به عنوان مثبت پیش‌بینی کرده است به کل تعداد نمونه‌هایی که مدل به عنوان مثبت پیش‌بینی کرده است.
#     خلاصه: نشان می‌دهد از بین تمام نمونه‌هایی که مدل به عنوان مثبت شناسایی کرده، چند مورد واقعاً مثبت بوده‌اند.
#     کاربرد: مهم است وقتی که هزینه مثبت اشتباه (False Positive) بالا است.
#      مثلا سالم ها را بیمار تشخیص داده
#         از بین مواردی که مدل مثبت تشخیص داده، چند درصد واقعا مثبت بوده اند.

#
"""# 4. Recall Score (امتیاز بازخوانی یا حساسیت):"""
#
#     تعریف: نسبت تعداد نمونه‌هایی که مدل به درستی به عنوان مثبت پیش‌بینی کرده است به کل تعداد نمونه‌های مثبت واقعی.
#     خلاصه: نشان می‌دهد مدل چه نسبتی از تمام نمونه‌های مثبت را توانسته شناسایی کند.
#     کاربرد: مهم است وقتی که هزینه منفی اشتباه (False Negative) بالا است.
#         یعنی اگر  پایین باشه مدل، بیمار را سالم تشخیص داده که خیلی بده.
#         از بین مواردی که واقعا مثبت بوده است چند درصد را مدل درست پیدا کرده

#

###_____________________
"""accuracy_score"""

from sklearn.metrics import accuracy_score
acc_train = accuracy_score(y_true= y_train_stratified, y_pred=y_pred_train)
acc_test = accuracy_score(y_true= y_test_stratified, y_pred=y_pred_test)

print(acc_train , acc_test)

###_____________________

""" confusion_matrix , precision_score , recall_score"""
from sklearn.metrics import confusion_matrix, precision_score, recall_score
c= confusion_matrix(y_train_stratified,y_pred_train)
p= precision_score(y_train_stratified,y_pred_train)
r = recall_score(y_train_stratified,y_pred_train)
print(c)
print( p , r)


###_______________________________________________________________________________________________

"""2. الگوریتم KNN (K-Nearest Neighbors): """


#       -الگوریتم K-Nearest Neighbors (KNN) یک الگوریتم یادگیری ماشین نظارت شده است
#        که برای مسائل دسته‌بندی و رگرسیون استفاده می‌شود. یعنی برای هر نمونه به تعداد  K همسایگی نزدیک را پیدا میکند
#       این الگوریتم( غیرپارامتری) است، به این معنی که هیچ فرضی در مورد توزیع داده‌ها نمی‌کند.
#
#
# توضیح بیشتر:
#
#     وقتی می‌گوییم یک الگوریتم یادگیری ماشین "هیچ فرضی در مورد توزیع داده‌ها نمی‌کند"، منظور این است که الگوریتم برای کار کردن به این نیاز ندارد که داده‌ها از یک توزیع احتمال خاص (مانند توزیع نرمال یا توزیع گاوسی) پیروی کنند.
#
#
#
#
# مثلا:
#
#     مثلا اگر دسته های مختلف از مثلا گوی قرمز و سبز  داشته باشیم
#     و یک نقطه (مکان داده) را در جایی بگذاریم
#     اگر k را مثلا 2 بذاریم. میگرده تا دو تا همسایگی برای ان نقطه در نظر بگیرد
#     و اگر هر دو همسایکی قرمز بود یعنی ان نقطه از کلاس قرمز محسوب شود
#     وقتی با تساوی روبرو می‌شویم، می‌توان از روش‌های مختلفی برای تعیین کلاس استفاده کرد:
#     افزایش K: ساده‌ترین راه این است که K را افزایش دهیم. اگر K را به 3 تغییر دهیم، ممکن است همسایه سوم کلاس را مشخص کند.
#     استفاده از وزن‌دهی: می‌توان به همسایه‌های نزدیک‌تر وزن بیشتری داد. در این صورت، اگر همسایه قرمز نزدیک‌تر از همسایه سبز باشد، ممکن است نقطه جدید به کلاس قرمز نسبت داده شود.
#     تصمیم‌گیری تصادفی: در صورت تساوی، می‌توان به صورت تصادفی یکی از کلاس‌ها را انتخاب کرد.
#     استفاده از الگوریتم‌های پیشرفته‌تر: روش‌های پیچیده‌تری برای حل تساوی وجود دارند که ممکن است در شرایط خاص استفاده شوند.
#
#
#
# جامعیت:
#
#     انتخاب K: انتخاب مقدار مناسب برای K بسیار مهم است. اگر K خیلی کوچک باشد، الگوریتم ممکن است به نویز حساس باشد. اگر K خیلی بزرگ باشد، الگوریتم ممکن است مرزهای تصمیم‌گیری را هموار کند و جزئیات مهم را از دست بدهد. معمولاً از روش‌هایی مانند اعتبارسنجی متقابل برای یافتن بهترین مقدار K استفاده می‌شود.
#     اندازه فاصله: انتخاب اندازه فاصله مناسب نیز مهم است و به نوع داده‌ها بستگی دارد. فاصله اقلیدسی برای داده‌های پیوسته و فاصله منهتن برای داده‌هایی که تفاوت بین ابعاد مهم است، مناسب هستند.
#     وزن‌دهی به همسایه‌ها: می‌توان به همسایه‌های نزدیک‌تر وزن بیشتری داد تا تأثیر بیشتری در تصمیم‌گیری داشته باشند.
#
# خلاصه:
#
#     الگوریتم KNN یک روش ساده و شهودی برای دسته‌بندی و رگرسیون است. این الگوریتم با یافتن K نزدیک‌ترین نمونه در مجموعه داده آموزشی به نمونه جدید، کلاس یا مقدار آن را تعیین می‌کند. انتخاب K و اندازه فاصله مناسب از عوامل کلیدی در عملکرد این الگوریتم هستند.
#
#
# مزایا:
#
#     ساده و آسان برای درک و پیاده‌سازی.
#     غیرپارامتری است و نیازی به فرضیات در مورد توزیع داده‌ها ندارد.
#     می‌تواند برای مسائل دسته‌بندی و رگرسیون استفاده شود.
#     در صورت وجود داده‌های آموزشی کافی، می‌تواند عملکرد خوبی داشته باشد.
#
# معایب:
#
#     محاسبات سنگین است، به خصوص برای داده‌های بزرگ، زیرا باید فاصله بین نمونه جدید و تمام نمونه‌های آموزشی محاسبه شود.
#     به انتخاب مقدار مناسب برای K حساس است.
#     ممکن است در داده‌های با ابعاد بالا عملکرد خوبی نداشته باشد (مشکل نفرین ابعاد).
#     به مقیاس ویژگی‌ها حساس است و نیاز به نرمال‌سازی داده‌ها دارد.
#
#
# توضیح کلی در مورد الگوریتم ها:
    # الگوریتم KNN یک الگوریتم غیرپارامتری است.
# این به این معنی است که KNN هیچ فرضی در مورد توزیع احتمال داده‌ها نمی‌کند.
# به جای آن، مستقیماً از داده‌های آموزشی برای پیش‌بینی استفاده می‌کند.
    # اما الگوریتم‌های پارامتری  بر خلاف KNN، فرض می‌کنند که داده‌ها از یک توزیع احتمال مشخص پیروی می‌کنند
# و هدف آن‌ها یادگیری پارامترهای این توزیع است.
###_____________________

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=4)
knn.fit(X_train_stratified,y_train_stratified)

y_pred_train=knn.predict(X_train_stratified)
y_pred_test=knn.predict(X_test_stratified)

###_____________________

"""# confusion_matrix , accuracy_score , precision_score , recall_score"""

acc_train = accuracy_score(y_true= y_train_stratified, y_pred=y_pred_train)
acc_test = accuracy_score(y_true= y_test_stratified, y_pred=y_pred_test)
acc_train , acc_test

confusion_matrix(y_test_stratified,y_pred_test)
###_____________________

p = precision_score(y_train_stratified,y_pred_train)
r= recall_score(y_train_stratified,y_pred_train)

print(p , r)

###_______________________________________________________________________________________________

"""درخت تصمیم (Decision Tree)"""
# توضیح کامل و خلاصه:
#
#     درخت تصمیم یک الگوریتم یادگیری ماشین نظارت شده است که برای مسائل دسته‌بندی و رگرسیون استفاده می‌شود. این الگوریتم یک ساختار درختی ایجاد می‌کند که در آن هر گره داخلی نشان‌دهنده یک آزمایش بر روی یک ویژگی (attribute) است، هر شاخه نشان‌دهنده نتیجه آزمایش است، و هر گره برگ نشان‌دهنده یک کلاس یا یک مقدار پیش‌بینی شده است.
#
# نحوه کار:
#
#     انتخاب بهترین ویژگی: الگوریتم با انتخاب بهترین ویژگی برای تقسیم داده‌ها شروع می‌کند. بهترین ویژگی معمولاً ویژگی است که بیشترین اطلاعات را به دست می‌دهد یا ناخالصی را کاهش می‌دهد (مانند استفاده از معیارهایی مانند آنتروپی یا شاخص جینی).
#     تقسیم داده‌ها: داده‌ها بر اساس مقدار انتخاب شده برای ویژگی، به زیرمجموعه‌ها تقسیم می‌شوند.
#     تکرار: این فرآیند به صورت بازگشتی برای هر زیرمجموعه تکرار می‌شود تا زمانی که یک شرط توقف برآورده شود (مانند رسیدن به یک عمق مشخص، داشتن تعداد نمونه‌های کم در یک گره، یا خالص شدن گره).
#     پیش‌بینی: برای پیش‌بینی یک نمونه جدید، الگوریتم از ریشه درخت شروع می‌کند و بر اساس مقدار ویژگی‌های نمونه، مسیر مناسب را در درخت دنبال می‌کند تا به یک گره برگ برسد. مقدار یا کلاس موجود در گره برگ به عنوان پیش‌بینی در نظر گرفته می‌شود.
#
# مزایا:
#
#     ساده و آسان برای درک و تفسیر.
#     نیاز به پیش‌پردازش کمی دارد.
#     می‌تواند داده‌های عددی و دسته‌ای را مدیریت کند.
#     می‌تواند روابط غیرخطی بین ویژگی‌ها را مدل کند.
#
# معایب:
#
#     مستعد بیش‌برازش (overfitting) است.
#     ممکن است به تغییرات کوچک در داده‌ها حساس باشد.
#     می‌تواند ناپایدار باشد (تغییرات کوچک در داده‌ها می‌تواند منجر به درخت‌های بسیار متفاوت شود).

###_____________________

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(max_depth=5, min_samples_split=2, min_samples_leaf=2)
dt.fit(X_train_stratified,y_train_stratified)

# # max_depth=5: این پارامتر حداکثر عمق درخت را تعیین می‌کند. عمق درخت تعداد گره‌ها از ریشه تا عمیق‌ترین برگ است. محدود کردن عمق درخت به کنترل پیچیدگی مدل کمک می‌کند و از بیش‌برازش (overfitting) جلوگیری می‌کند. مقدار 5 به این معنی است که درخت حداکثر می‌تواند 5 سطح داشته باشد.
#
# min_samples_split=2: این پارامتر حداقل تعداد نمونه‌های مورد نیاز برای تقسیم یک گره داخلی را تعیین می‌کند. اگر یک گره داخلی تعداد نمونه‌های کمتری از این مقدار داشته باشد، دیگر تقسیم نخواهد شد. این پارامتر نیز برای جلوگیری از بیش‌برازش استفاده می‌شود. مقدار 2 به این معنی است که یک گره داخلی باید حداقل 2 نمونه داشته باشد تا بتواند تقسیم شود.
#
# min_samples_leaf=2: این پارامتر حداقل تعداد نمونه‌های مورد نیاز برای قرار گرفتن در یک گره برگ را تعیین می‌کند. یک گره برگ باید حداقل این تعداد نمونه را داشته باشد. این پارامتر به جلوگیری از ایجاد برگ‌های خیلی کوچک و حساس به نویز کمک می‌کند. مقدار 2 به این معنی است که هر گره برگ باید حداقل 2 نمونه داشته باشد.


"""جنگل تصادفی (Random Forest)"""

# توضیح کامل و خلاصه:
#
#     جنگل تصادفی یک الگوریتم یادگیری ماشین نظارت شده است که از چندین درخت تصمیم استفاده می‌کند. این الگوریتم با ایجاد تعداد زیادی درخت تصمیم به صورت تصادفی و سپس جمع‌آوری پیش‌بینی‌های آن‌ها، تلاش می‌کند تا دقت و پایداری مدل را بهبود بخشد.
#
# نحوه کار:
#
#     نمونه‌گیری بوت استرپ (Bootstrap Sampling): الگوریتم به طور تصادفی نمونه‌هایی را با جایگزینی از مجموعه داده آموزشی انتخاب می‌کند تا مجموعه‌های داده جدید برای آموزش هر درخت ایجاد کند.
#     انتخاب ویژگی تصادفی: هنگام ساخت هر درخت، به جای در نظر گرفتن تمام ویژگی‌ها برای تقسیم یک گره، تنها یک زیرمجموعه تصادفی از ویژگی‌ها در نظر گرفته می‌شود.
#     ساخت درخت: هر درخت تصمیم با استفاده از مجموعه داده بوت استرپ و زیرمجموعه تصادفی ویژگی‌ها ساخته می‌شود.
#     تجمیع پیش‌بینی‌ها: برای پیش‌بینی یک نمونه جدید، هر درخت در جنگل یک پیش‌بینی انجام می‌دهد. برای دسته‌بندی، پیش‌بینی‌ها با استفاده از رای‌گیری اکثریت (majority voting) تجمیع می‌شوند. برای رگرسیون، میانگین پیش‌بینی‌ها محاسبه می‌شود.
#
# مزایا:
#
#     معمولاً دقت بالاتری نسبت به یک درخت تصمیم واحد دارد.
#     کمتر مستعد بیش‌برازش است.
#     می‌تواند ویژگی‌های مهم را شناسایی کند.
#     برای داده‌های بزرگ مناسب است.
#
# معایب:
#
#     تفسیر آن دشوارتر از یک درخت تصمیم واحد است.
#     آموزش آن ممکن است زمان بیشتری ببرد.
###_____________________

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100 , max_depth=8)
rf.fit(X_train_stratified,y_train_stratified)

# n_estimators=100: این پارامتر تعداد درخت‌های تصمیم در جنگل را تعیین می‌کند. هرچه تعداد درخت‌ها بیشتر باشد، مدل معمولاً پایدارتر و دقیق‌تر می‌شود. با این حال، افزایش بیش از حد تعداد درخت‌ها ممکن است زمان آموزش را افزایش دهد و بهبود قابل توجهی در عملکرد ایجاد نکند. مقدار 100 به این معنی است که جنگل از 100 درخت تصمیم تشکیل شده است.

# max_depth=8: این پارامتر حداکثر عمق هر درخت تصمیم در جنگل را تعیین می‌کند. مانند پارامتر max_depth در DecisionTreeClassifier, این پارامتر به کنترل پیچیدگی هر درخت و جلوگیری از بیش‌برازش کمک می‌کند. مقدار 8 به این معنی است که هر درخت در جنگل حداکثر می‌تواند 8 سطح داشته باشد.
###_____________________

""" confusion_matrix , accuracy_score , precision_score , recall_score """

"""accuracy_score"""

y_pred_train_dt =dt.predict(X_train_stratified)
y_pred_train_rf=dt.predict(X_train_stratified)

acc_train_dt= accuracy_score (y_train_stratified,y_pred_train_dt)
acc_train_rf= accuracy_score (y_train_stratified,y_pred_train_dt)

print(acc_train_dt , acc_train_rf)
###_____________________

"""accuracy_score"""

y_pred_test_dt =dt.predict(X_test_stratified)
y_pred_test_rf=dt.predict(X_test_stratified)

acc_test_dt= accuracy_score (y_test_stratified,y_pred_test_dt)
acc_test_rf= accuracy_score (y_test_stratified,y_pred_test_dt)

print(acc_test_dt , acc_test_rf)
###_____________________

"""recall_score"""
r_dt =recall_score(y_test_stratified,y_pred_test_dt)
r_rf =recall_score(y_test_stratified,y_pred_test_rf)

print(r_dt , r_rf)
###_____________________

"""precision_score"""
p_dt =precision_score(y_test_stratified,y_pred_test_dt)
p_dr= precision_score(y_test_stratified,y_pred_test_rf)

print(p_dt , p_dr)
###_______________________________________________________________________________________________

"""آشنایی و پیاده سازی الگوریتم ماشین بردار پشتیبان (SVM)
"""

# ماشین بردار پشتیبان (SVM) یک الگوریتم یادگیری ماشین نظارت شده است که برای مسائل دسته‌بندی و رگرسیون استفاده می‌شود. هدف اصلی SVM یافتن یک ابرصفحه (hyperplane) بهینه است که داده‌ها را به بهترین شکل ممکن جدا کند.
#
# توضیح خلاصه:
#
# SVM سعی می‌کند یک خط (یا یک ابرصفحه در ابعاد بالاتر) پیدا کند که نه تنها داده‌ها را جدا کند، بلکه حاشیه (margin) بین داده‌ها و ابرصفحه را نیز حداکثر کند. این حاشیه حداکثری باعث می‌شود که مدل در پیش‌بینی داده‌های جدید عملکرد بهتری داشته باشد و کمتر به داده‌های آموزشی وابسته باشد (کاهش بیش‌برازش).
#
# توضیح جامع:
#
#     ابرصفحه (Hyperplane): در یک فضای n بعدی، یک ابرصفحه یک فضای n-1 بعدی است که داده‌ها را جدا می‌کند. برای مثال، در یک فضای دو بعدی، ابرصفحه یک خط است.
#     حاشیه (Margin): فاصله بین ابرصفحه و نزدیک‌ترین نقاط داده از هر کلاس. هدف SVM حداکثر کردن این حاشیه است.
#     بردارهای پشتیبان (Support Vectors): نقاط داده‌ای که نزدیک‌ترین فاصله را به ابرصفحه دارند و در تعیین موقعیت و جهت ابرصفحه نقش کلیدی دارند.
#     هسته (Kernel): SVM می‌تواند از توابع هسته برای حل مسائل غیرخطی استفاده کند. توابع هسته داده‌ها را به یک فضای بالاتر منتقل می‌کنند که در آن می‌توانند به صورت خطی جدا شوند. هسته‌های رایج شامل هسته خطی، هسته چندجمله‌ای، و هسته تابع شعاعی (RBF) است.
#     مسئله بهینه‌سازی: SVM به یک مسئله بهینه‌سازی تبدیل می‌شود که هدف آن یافتن ابرصفحه با حداکثر حاشیه است. این مسئله معمولاً با استفاده از تکنیک‌های بهینه‌سازی حل می‌شود.
#
# مزایا:
#
#     عملکرد خوب در فضاهای با ابعاد بالا.
#     استفاده از هسته‌ها برای حل مسائل غیرخطی.
#     مقاومت در برابر بیش‌برازش به دلیل حداکثر کردن حاشیه.
#
# معایب:
#
#     محاسبات سنگین برای مجموعه‌های داده بزرگ.
#     انتخاب هسته و پارامترهای مناسب می‌تواند چالش‌برانگیز باشد.
#     عملکرد ضعیف در مجموعه‌های داده بسیار نویزی.

# اس وی ام انتظار داره که خط یا صفحه یا .. به عنوان مرزی بین کلاسها، بیشترین فاصله را  از نقاط کلاسها داشته باشه

###_____________________

from sklearn import svm
model = svm.SVC(kernel='linear')
model.fit(X_train_stratified,y_train_stratified)
###_____________________

y_pred_train=model.predict(X_train_stratified)
y_pred_test=model.predict(X_test_stratified)
###_____________________

from sklearn.metrics import confusion_matrix, precision_score, recall_score , accuracy_score
acc_train = accuracy_score(y_true= y_train_stratified, y_pred=y_pred_train)
acc_test = accuracy_score(y_true= y_test_stratified, y_pred=y_pred_test)
print(acc_train , acc_test)

###_____________________

p = precision_score(y_train_stratified,y_pred_train)
r = recall_score(y_train_stratified,y_pred_train)
print(p , r)

###_______________________________________________________________________________________________

""" الگوریتم Logistic Regression"""
